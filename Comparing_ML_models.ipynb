{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025e98b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning models and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better plots\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b37f26",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "lr_report = classification_report(y_test, lr_predictions, \n",
    "                                target_names=data.target_names, \n",
    "                                digits=4)\n",
    "\n",
    "lr_report_dict = classification_report(y_test, lr_predictions, \n",
    "                                     target_names=data.target_names, \n",
    "                                     output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b47abae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dt_report = classification_report(y_test, dt_predictions, \n",
    "                                target_names=data.target_names, \n",
    "                                digits=4)\n",
    "\n",
    "dt_report_dict = classification_report(y_test, dt_predictions, \n",
    "                                     target_names=data.target_names, \n",
    "                                     output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e59487",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, model_name, class_names):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix with proper labels and formatting\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix for Logistic Regression\n",
    "plot_confusion_matrix(y_test, lr_predictions, \n",
    "                     \"Logistic Regression\", data.target_names)\n",
    "\n",
    "# Plot confusion matrix for Decision Tree\n",
    "plot_confusion_matrix(y_test, dt_predictions, \n",
    "                     \"Decision Tree\", data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a27c5a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_prob, model_name):\n",
    "    \"\"\"\n",
    "    Plot ROC curve and calculate AUC score\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'{model_name} (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "             label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return roc_auc\n",
    "\n",
    "# Plot ROC curve for Logistic Regression\n",
    "lr_auc = plot_roc_curve(y_test, lr_probabilities, \"Logistic Regression\")\n",
    "\n",
    "# Plot ROC curve for Decision Tree\n",
    "dt_auc = plot_roc_curve(y_test, dt_probabilities, \"Decision Tree\")\n",
    "\n",
    "# Compare both models on the same plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Logistic Regression ROC\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probabilities)\n",
    "plt.plot(lr_fpr, lr_tpr, color='blue', lw=2, \n",
    "         label=f'Logistic Regression (AUC = {lr_auc:.4f})')\n",
    "\n",
    "# Decision Tree ROC\n",
    "dt_fpr, dt_tpr, _ = roc_curve(y_test, dt_probabilities)\n",
    "plt.plot(dt_fpr, dt_tpr, color='red', lw=2, \n",
    "         label=f'Decision Tree (AUC = {dt_auc:.4f})')\n",
    "\n",
    "# Random classifier line\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', \n",
    "         label='Random Classifier (AUC = 0.5)')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f64fc1",
   "metadata": {},
   "source": [
    "Comprehensive Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e6e16",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_model_comparison(models_data):\n",
    "    \"\"\"\n",
    "    Create a comprehensive comparison of multiple models\n",
    "    \"\"\"\n",
    "    comparison_df = pd.DataFrame(models_data)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Prepare comparison data\n",
    "models_comparison = {\n",
    "    'Model': ['Logistic Regression', 'Decision Tree'],\n",
    "    'Precision (Malignant)': [\n",
    "        lr_report_dict['malignant']['precision'],\n",
    "        dt_report_dict['malignant']['precision']\n",
    "    ],\n",
    "    'Recall (Malignant)': [\n",
    "        lr_report_dict['malignant']['recall'],\n",
    "        dt_report_dict['malignant']['recall']\n",
    "    ],\n",
    "    'F1-Score (Malignant)': [\n",
    "        lr_report_dict['malignant']['f1-score'],\n",
    "        dt_report_dict['malignant']['f1-score']\n",
    "    ],\n",
    "    'Overall Accuracy': [\n",
    "        lr_report_dict['accuracy'],\n",
    "        dt_report_dict['accuracy']\n",
    "    ],\n",
    "    'AUC Score': [lr_auc, dt_auc]\n",
    "}\n",
    "\n",
    "comparison_df = create_model_comparison(models_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aeb419",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a bar plot comparing key metrics\n",
    "metrics = ['Precision (Malignant)', 'Recall (Malignant)', 'F1-Score (Malignant)', 'Overall Accuracy', 'AUC Score']\n",
    "lr_scores = [comparison_df.iloc[0][metric] for metric in metrics]\n",
    "dt_scores = [comparison_df.iloc[1][metric] for metric in metrics]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "bars1 = ax.bar(x - width/2, lr_scores, width, label='Logistic Regression', color='skyblue')\n",
    "bars2 = ax.bar(x + width/2, dt_scores, width, label='Decision Tree', color='lightcoral')\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# Add value labels on bars\n",
    "def add_value_labels(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "add_value_labels(bars1)\n",
    "add_value_labels(bars2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78922c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def recommend_best_model(comparison_df):\n",
    "    \"\"\"\n",
    "    Provide model recommendation based on performance metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"MODEL SELECTION RECOMMENDATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Analyze each metric\n",
    "    lr_row = comparison_df.iloc[0]\n",
    "    dt_row = comparison_df.iloc[1]\n",
    "    \n",
    "    print(\"Metric Analysis:\")\n",
    "    print(f\"• Precision (Malignant): {'Logistic Regression' if lr_row['Precision (Malignant)'] > dt_row['Precision (Malignant)'] else 'Decision Tree'} wins\")\n",
    "    print(f\"• Recall (Malignant): {'Logistic Regression' if lr_row['Recall (Malignant)'] > dt_row['Recall (Malignant)'] else 'Decision Tree'} wins\")\n",
    "    print(f\"• F1-Score (Malignant): {'Logistic Regression' if lr_row['F1-Score (Malignant)'] > dt_row['F1-Score (Malignant)'] else 'Decision Tree'} wins\")\n",
    "    print(f\"• Overall Accuracy: {'Logistic Regression' if lr_row['Overall Accuracy'] > dt_row['Overall Accuracy'] else 'Decision Tree'} wins\")\n",
    "    print(f\"• AUC Score: {'Logistic Regression' if lr_row['AUC Score'] > dt_row['AUC Score'] else 'Decision Tree'} wins\")\n",
    "    \n",
    "    # Overall recommendation\n",
    "    lr_wins = sum([\n",
    "        lr_row['Precision (Malignant)'] > dt_row['Precision (Malignant)'],\n",
    "        lr_row['Recall (Malignant)'] > dt_row['Recall (Malignant)'],\n",
    "        lr_row['F1-Score (Malignant)'] > dt_row['F1-Score (Malignant)'],\n",
    "        lr_row['Overall Accuracy'] > dt_row['Overall Accuracy'],\n",
    "        lr_row['AUC Score'] > dt_row['AUC Score']\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\nOverall Winner: {'Logistic Regression' if lr_wins > 2 else 'Decision Tree'}\")\n",
    "    print(f\"Logistic Regression wins in {lr_wins}/5 metrics\")\n",
    "    print(f\"Decision Tree wins in {5-lr_wins}/5 metrics\")\n",
    "    \n",
    "    # Context-specific recommendations\n",
    "    print(\"\\nContext-Specific Recommendations:\")\n",
    "    print(\"• For medical diagnosis (high recall important): Choose the model with higher recall\")\n",
    "    print(\"• For balanced performance: Choose the model with higher F1-score\")\n",
    "    print(\"• For probability estimates: Choose the model with higher AUC score\")\n",
    "\n",
    "recommend_best_model(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ab7616",
   "metadata": {},
   "source": [
    "Advanced Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f38dd0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze feature importance for Decision Tree\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': dt_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features (Decision Tree):\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 15 Feature Importances - Decision Tree')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944e9a99",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze misclassified samples\n",
    "def analyze_errors(y_true, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Analyze misclassified samples\n",
    "    \"\"\"\n",
    "    errors = y_true != y_pred\n",
    "    error_count = np.sum(errors)\n",
    "    \n",
    "    print(f\"\\n{model_name} Error Analysis:\")\n",
    "    print(f\"Total misclassified samples: {error_count}\")\n",
    "    print(f\"Error rate: {error_count/len(y_true):.4f}\")\n",
    "    \n",
    "    # False positives and false negatives\n",
    "    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    print(f\"False Positives (Benign predicted as Malignant): {false_positives}\")\n",
    "    print(f\"False Negatives (Malignant predicted as Benign): {false_negatives}\")\n",
    "    \n",
    "    return errors\n",
    "\n",
    "lr_errors = analyze_errors(y_test, lr_predictions, \"Logistic Regression\")\n",
    "dt_errors = analyze_errors(y_test, dt_predictions, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e9b1b6",
   "metadata": {},
   "source": [
    "Troubleshooting Common Issues\n",
    "\n",
    "Issue 1: Import Errors\n",
    "If you encounter import errors, ensure all required packages are installed:\n",
    "# Run this if you get import errors\n",
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn pandas numpy matplotlib seaborn\n",
    "\n",
    "Issue 2: Memory Issues with Large Datasets\n",
    "For larger datasets, consider using sample data:\n",
    "# If working with large datasets, sample the data\n",
    "if X.shape[0] > 10000:\n",
    "    from sklearn.utils import resample\n",
    "    X_sample, y_sample = resample(X, y, n_samples=5000, random_state=42)\n",
    "    X, y = X_sample, y_sample\n",
    "\n",
    "Issue 3: Convergence Warnings\n",
    "If you get convergence warnings with Logistic Regression:\n",
    "# Increase max_iter or change solver\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=2000, solver='liblinear')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
